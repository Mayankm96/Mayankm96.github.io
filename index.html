<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Luyang Zhao</title>
  <meta name="description" content="A simple website for a Roboticist.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Miscellarnous -->
        <a class="page-link" href="/#publications">publications</a>
        <a class="page-link" href="/projects">projects</a>
        <a class="page-link" href="/tutorials">tutorials</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- CV link -->
        <a class="page-link" href="/assets/documents/Mayank_CV.pdf">vitae</a>

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">
<strong>Mayank</strong> Mittal</h1>
    <h5 class="post-description">Graduate Student | Department of Mechanical Engineering, ETH Zurich</h5>
  </header>

  <article class="post-content &lt;strong&gt;Mayank&lt;/strong&gt; Mittal clearfix">
    
  <div class="profile col one right">
    
      <img class="one" src="/assets/img/prof_pic2.jpg">
    
    
  </div>


<p><em>How can a robot learn from its own interactions?</em><br>
<em>What abstractions are necessary to describe a task?</em><br>
<em>When does a robot even know that the task is now completed?</em></p>

<p>In a quest to find answers to the above questions, I am currently a PhD student at <a href="https://ethz.ch/en.html">ETH Zurich</a> advised by <a href="http://www.rsl.ethz.ch/the-lab/people/person-detail.html?persid=121911">Marco Hutter</a>, and a Research Scientist at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>. I also closely collaborate with <a href="https://animesh.garg.tech/">Animesh Garg</a> at the <a href="https://web.cs.toronto.edu/">University of Toronto</a>.</p>

<p>Over the past few years, I have had the opportunity to work with some amazing robotic groups.
I have been a visiting student researcher at <a href="https://vectorinstitute.ai/">Vector Institute</a>,
a research intern at <a href="https://nnaisense.com/">NNAISENSE</a>, and a part-time research engineer
at <a href="https://ethz.ch/en.html">ETH Zurich</a>. During my undergrad at <a href="http://www.iitk.ac.in/ee/">IIT Kanpur</a>, I was a visiting student at
University of Freiburg, Germany, working closely with <a href="http://www2.informatik.uni-freiburg.de/~valada/">Abhinav Valada</a> and <a href="http://www2.informatik.uni-freiburg.de/~burgard/">Wolfram Burgard</a>.</p>

<p>I am incredibly thankful to my collaborators and mentors, and enjoy exploring new domains through collaborations. If you have questions or would like to work together, feel free to reach out through
<a href="mailto:mittalma@ethz.ch">email</a>!</p>

<!-- _Shameless promotion:_  
For undergrad/graduate students at [ETH Zurich](https://ethz.ch/en.html): In case you are looking for semester projects or master thesis, please check [here](https://rsl.ethz.ch/education-students.html) for available projects with me and other amazing people in our group! -->

<div class="post">

  
    <div class="news">
  <h2>news</h2>
  
    <table>
    
    
      <tr>
        <td class="date">Apr 20, 2023</td>
        <td class="announcement">
          
            Our paper on <em>‘Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments’</em> is accepted to IEEE RA-L and will be presented at <a href="https://ieee-iros.org/">IROS 2023</a>

          
        </td>
      </tr>
    
      <tr>
        <td class="date">Jul 1, 2022</td>
        <td class="announcement">
          
            Our papers on <a href="https://arxiv.org/abs/2103.10534">articulated object</a> and <a href="https://arxiv.org/abs/2108.09779">in-hand</a> manipulation are accepted to IROS 2022 <img class="emoji" title=":robot:" alt=":robot:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png" height="20" width="20">

          
        </td>
      </tr>
    
      <tr>
        <td class="date">Jan 31, 2022</td>
        <td class="announcement">
          
            Our paper on <em>‘A Collision-Free MPC for Whole-Body Dynamic Locomotion and Manipulation’</em> is accepted to <a href="https://www.icra2022.org/">ICRA 2022</a>

          
        </td>
      </tr>
    
      <tr>
        <td class="date">Oct 7, 2021</td>
        <td class="announcement">
          
            Joined <a href="https://rsl.ethz.ch/">Marco Hutter’s group</a> at ETH Zurich as a PhD student

          
        </td>
      </tr>
    
      <tr>
        <td class="date">Jun 28, 2021</td>
        <td class="announcement">
          
            Excited to start as a Deep Learning R&amp;D Engineer at NVIDIA!

          
        </td>
      </tr>
    
    </table>
  
</div>

  

</div>

<hr>

<h2 id="research-interests"><strong>research interests</strong></h2>

<p>I am primarily interested in the decision-making and control of robots in human environments.
These days, my efforts are focused on designing perception-based systems for contact-rich manipulation tasks, such as articulated object interaction with mobile manipulators and in-hand manipulation.
Other areas of interest include hierarchical reinforcement learning, optimal control, and 3D vision.</p>

<hr>

<h2 id="publications"><strong>publications</strong></h2>

<ol class="bibliography"><li>
<div id="mittal2023orbit" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="/assets/img/teasers/orbit.jpg">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">ORBIT: A Unified Simulation Framework for Interactive Robot Learning Environments</span>
      <span class="author">
        
          
            
              
                
                  Mayank Mittal,
                
              
            
          
        
          
            
              
                
                  Calvin Yu,
                
              
            
          
        
          
            
              
                
                  Qinxi Yu,
                
              
            
          
        
          
            
              
                
                  Jingzhou Liu,
                
              
            
          
        
          
            
              
                
                  <a href="https://ch.linkedin.com/in/nikita-rudin-bb3199121" target="_blank">Nikita Rudin</a>,
                
              
            
          
        
          
            
              
                
                  David Hoeller,
                
              
            
          
        
          
            
              
                
                  and  others
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em> (Under Review) </em>
      
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2301.04195" target="_blank">arXiv</a>]
    
    
    
      [<a href="https://isaac-orbit.github.io/" target="_blank">Website</a>]
    
    
    
    
    
    
      [<a href="https://github.com/NVIDIA-Omniverse/orbit" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We present ORBIT, a unified and modular framework for robot learning powered by NVIDIA Isaac Sim. It offers a modular design to easily and efficiently create robotic environments with photo-realistic scenes and fast and accurate rigid and deformable body simulation. With ORBIT, we provide a suite of benchmark tasks of varying difficulty – from single-stage cabinet opening and cloth folding to multi-stage tasks such as room reorganization. To support working with diverse observations and action spaces, we include fixed-arm and mobile manipulators with different physically-based sensors and motion generators. ORBIT allows training reinforcement learning policies and collecting large demonstration datasets from hand-crafted or expert solutions in a matter of minutes by leveraging GPU-based parallelization. In summary, we offer an open-sourced framework that readily comes with 16 robotic platforms, 4 sensor modalities, 10 motion generators, more than 20 benchmark tasks, and wrappers to 4 learning libraries. With this framework, we aim to support various research areas, including representation learning, reinforcement learning, imitation learning, and task and motion planning. We hope it helps establish interdisciplinary collaborations in these communities, and its modularity makes it easily extensible for more tasks and applications in the future.</p>
    </span>
    
  </div>
</div>
</li></ol>

<ol class="bibliography">
<li>
<div id="2202.12385" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="/assets/img/teasers/alma_self.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">A Collision-Free MPC for Whole-Body Dynamic Locomotion and Manipulation</span>
      <span class="author">
        
          
            
              
                
                  Jia-Ruei Chiu,
                
              
            
          
        
          
            
              
                
                  Jean-Pierre Sleiman,
                
              
            
          
        
          
            
              
                
                  Mayank Mittal,
                
              
            
          
        
          
            
              
                
                  <a href="https://scholar.google.ch/citations?user=ciragesAAAAJ&amp;hl=en" target="_blank">Farbod Farshidian</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://rsl.ethz.ch/" target="_blank">Marco Hutter</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ICRA</em>
      
      
        
          2022
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2202.12385" target="_blank">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=m3rJWJVzYuY" target="_blank">Video</a>]
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we present a real-time whole-body planner for collision-free legged mobile manipulation. We enforce both self-collision and environment-collision avoidance as soft constraints within a Model Predictive Control (MPC) scheme that solves a multi-contact optimal control problem. By penalizing the signed distances among a set of representative primitive collision bodies, the robot is able to safely execute a variety of dynamic maneuvers while preventing any self-collisions. Moreover, collision-free navigation and manipulation in both static and dynamic environments are made viable through efficient queries of distances and their gradients via a euclidean signed distance field. We demonstrate through a comparative study that our approach only slightly increases the computational complexity of the MPC planning. Finally, we validate the effectiveness of our framework through a set of hardware experiments involving dynamic mobile manipulation tasks with potential collisions, such as locomotion balancing with the swinging arm, weight throwing, and autonomous door opening.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div id="2108.09779" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="/assets/img/teasers/trifinger.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger</span>
      <span class="author">
        
          
            
              
                
                  <a href="https://allshire.org/" target="_blank">Arthur Allshire</a>,
                
              
            
          
        
          
            
              
                
                  Mayank Mittal,
                
              
            
          
        
          
            
              
                
                  Varun Lodaya,
                
              
            
          
        
          
            
              
                
                  Viktor Makoviychuk,
                
              
            
          
        
          
            
              
                
                  Denys Makoviichuk,
                
              
            
          
        
          
            
              
                
                  Felix Widmaier,
                
              
            
          
        
          
            
              
                
                  Manuel Wüthrich,
                
              
            
          
        
          
            
              
                
                  Stefan Bauer,
                
              
            
          
        
          
            
              
                
                  <a href="https://luyangzhao.github.io/" target="_blank">Ankur Handa</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="http://animesh.garg.tech/" target="_blank">Animesh Garg</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IROS</em>
      
      
        
          2022
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2108.09779" target="_blank">arXiv</a>]
    
    
    
      [<a href="https://s2r2-ig.github.io/" target="_blank">Website</a>]
    
    
    
    
    
    
      [<a href="https://github.com/pairlab/leibnizgym" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We present a system for learning a challenging dexterous manipulation task involving moving a cube to an arbitrary 6-DoF pose with only 3-fingers trained with NVIDIA’s IsaacGym simulator. We show empirical benefits, both in simulation and sim-to-real transfer, of using keypoints as opposed to position+quaternion representations for the object pose in 6-DoF for policy observations and in reward calculation to train a model-free reinforcement learning agent. By utilizing domain randomization strategies along with the keypoint representation of the pose of the manipulated object, we achieve a high success rate of 83% on a remote TriFinger system maintained by the organizers of the Real Robot Challenge. With the aim of assisting further research in learning in-hand manipulation, we make the codebase of our system, along with trained checkpoints that come with billions of steps of experience available, available publicly.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div id="2002.10451" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="/assets/img/teasers/alma_dishwasher.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Articulated Object Interaction in Unknown Scenes with Whole-Body Mobile Manipulation</span>
      <span class="author">
        
          
            
              
                
                  Mayank Mittal,
                
              
            
          
        
          
            
              
                
                  David Hoeller,
                
              
            
          
        
          
            
              
                
                  <a href="https://scholar.google.ch/citations?user=ciragesAAAAJ&amp;hl=en" target="_blank">Farbod Farshidian</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://rsl.ethz.ch/" target="_blank">Marco Hutter</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="http://animesh.garg.tech/" target="_blank">Animesh Garg</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IROS</em>
      
      
        
          2022
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2103.10534" target="_blank">arXiv</a>]
    
    
    
      [<a href="https://www.pair.toronto.edu/articulated-mm/" target="_blank">Website</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>A kitchen assistant needs to operate human-scale objects, such as cabinets and ovens, in unmapped environments with dynamic obstacles. Autonomous interactions in such real-world environments require integrating dexterous manipulation and fluid mobility. While mobile manipulators in different form-factors provide an extended workspace, their real-world adoption has been limited. This limitation is in part due to two main reasons: 1) inability to interact with unknown human-scale objects such as cabinets and ovens, and 2) inefficient coordination between the arm and the mobile base. Executing a high-level task for general objects requires a perceptual understanding of the object as well as adaptive whole-body control among dynamic obstacles. In this paper, we propose a two-stage architecture for autonomous interaction with large articulated objects in unknown environments. The first stage uses a learned model to estimate the articulated model of a target object from an RGB-D input and predicts an action-conditional sequence of states for interaction. The second stage comprises of a whole-body motion controller to manipulate the object along the generated kinematic plan. We show that our proposed pipeline can handle complicated static and dynamic kitchen settings. Moreover, we demonstrate that the proposed approach achieves better performance than commonly used control methods in mobile manipulation.</p>
    </span>
    
  </div>
</div>
</li>
</ol>

<ol class="bibliography"></ol>

<ol class="bibliography">
<li>
<div id="2002.10452" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="/assets/img/teasers/lyap_mpc.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Neural Lyapunov Model Predictive Control</span>
      <span class="author">
        
          
            
              
                
                  Mayank Mittal,
                
              
            
          
        
          
            
              
                
                  <a href="https://ch.linkedin.com/in/marco-gallieri-166a0421" target="_blank">Marco Gallieri</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://ch.linkedin.com/in/aqua83" target="_blank">Alessio Quaglino</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://ch.linkedin.com/in/seyed-sina-mirrazavi-salehian-11772856" target="_blank">Seyed Sina Mirrazavi Salehian</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="http://www.idsia.ch/~koutnik" target="_blank">Jan Koutnik</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em> (Under Review) </em>
      
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2002.10451" target="_blank">arXiv</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper presents <i>Neural Lyapunov MPC</i>, an algorithm to alternately train a Lyapunov neural network and a stabilising constrained Model Predictive Controller (MPC), given a neural network model of the system dynamics. This extends recent works on Lyapunov networks to be able to train solely from expert demonstrations of one-step transitions. The learned Lyapunov network is used as the value function for the MPC in order to guarantee stability and extend the stable region. Formal results are presented on the existence of a set of MPC parameters, such as discount factors, that guarantees stability with a horizon as short as one. Robustness margins are also discussed and existing performance bounds on value function MPC are extended to the case of imperfect models. The approach is tested on unstable non-linear continuous control tasks with hard constraints. Results demonstrate that, when a neural network trained on short sequences is used for predictions, a one-step horizon Neural Lyapunov MPC can successfully reproduce the expert behaviour and significantly outperform longer horizon MPCs.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div id="2001.01234" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="/assets/img/teasers/miscalib.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Learning Camera Miscalibration Detection</span>
      <span class="author">
        
          
            
              
                
                  <a href="https://asl.ethz.ch/the-lab/people/person-detail.MjMxMjQw.TGlzdC8xNTg0LDEyMDExMzk5Mjg=.html" target="_blank">Andrei Cramariuc</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.linkedin.com/in/aleksandar-petrov/" target="_blank">Aleksandar Petrov</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.linkedin.com/in/rohit-suri-0966b0b3/" target="_blank">Rohit Suri</a>,
                
              
            
          
        
          
            
              
                
                  Mayank Mittal,
                
              
            
          
        
          
            
              
                
                  <a href="https://mavt.ethz.ch/the-department/people/person-detail.Mjk5ODE=.TGlzdC81NTMsLTY5MzYxOTMw.html" target="_blank">Roland Siegwart</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="http://n.ethz.ch/~cesarc/" target="_blank">Cesar Cadena</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ICRA</em>
      
      
        
          2020
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2005.11711" target="_blank">arXiv</a>]
    
    
    
    
    
    
    
    
      [<a href="http://github.com/ethz-asl/camera_miscalib_detection" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Self-diagnosis and self-repair are some of the key challenges in deploying robotic platforms for long-term real-world applications. One of the issues that can occur to a robot is miscalibration of its sensors due to aging, environmental transients or external disturbances. Precise calibration lies at the core of a variety of applications, due to the need to accurately perceive the world. However, while a lot of work has focused on calibrating the sensors, not much has been done towards identifying when a sensor needs to be recalibrated. In this paper, we focus on a data-driven approach to learn the detection of miscalibration in vision sensors, specifically RGB cameras. Our contributions include a proposed miscalibration metric for RGB cameras and a novel semi-synthetic dataset generation pipeline based on this metric. Additionally, by training a deep convolutional neural network, we demonstrate the effectiveness of our pipeline to identify whether a recalibration of the camera’s intrinsic parameters is required or not.</p>
    </span>
    
  </div>
</div>
</li>
</ol>

<ol class="bibliography"><li>
<div id="1906.01304" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="/assets/img/teasers/vision_uav.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Vision-based Autonomous UAV Navigation and Landing for Urban Search and Rescue</span>
      <span class="author">
        
          
            
              
                
                  Mayank Mittal,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.linkedin.com/in/rohit-mohan-508112142/" target="_blank">Rohit Mohan</a>,
                
              
            
          
        
          
            
              
                
                  <a href="http://www2.informatik.uni-freiburg.de/~burgard/" target="_blank">Wolfram Burgard</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="http://www2.informatik.uni-freiburg.de/~valada/" target="_blank">Abhinav Valada</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ISRR</em>
      
      
        
          2019
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/1906.01304" target="_blank">arXiv</a>]
    
    
    
      [<a href="http://autoland.cs.uni-freiburg.de" target="_blank">Website</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving technology that can enable identification of survivors under collapsed buildings in the aftermath of natural disasters such as earthquakes or gas explosions. However, these UAVs have to be able to autonomously navigate in disaster struck environments and land on debris piles in order to accurately locate the survivors. This problem is extremely challenging as pre-existing maps cannot be leveraged for navigation due to structural changes that may have occurred and existing landing site detection algorithms are not suitable to identify safe landing regions on debris piles. In this work, we present a computationally efficient system for autonomous UAV navigation and landing that does not require any prior knowledge about the environment. We propose a novel landing site detection algorithm that computes costmaps based on several hazard factors including terrain flatness, steepness, depth accuracy, and energy consumption information. We also introduce a first-of-a-kind synthetic dataset of over 1.2 million images of collapsed buildings with groundtruth depth, surface normals, semantics and camera pose information. We demonstrate the efficacy of our system using experiments from a city scale hyperrealistic simulation environment and in real-world scenarios with collapsed buildings.</p>
    </span>
    
  </div>
</div>
</li></ol>

<ol class="bibliography"><li>
<div id="1809.05700" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="/assets/img/teasers/landing.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Vision-based Autonomous Landing in Catastrophe-Struck Environments</span>
      <span class="author">
        
          
            
              
                
                  Mayank Mittal,
                
              
            
          
        
          
            
              
                
                  <a href="http://www2.informatik.uni-freiburg.de/~valada/" target="_blank">Abhinav Valada</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="http://www2.informatik.uni-freiburg.de/~burgard/" target="_blank">Wolfram Burgard</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>Workshop on Vision-based Drones: What’s Next?</em>
        <br>
        <em>IROS</em>
      
      
        
          2018
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/1809.05700" target="_blank">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=6FEgU0kYqlM" target="_blank">Video</a>]
    
    
    
      [<a href="/assets/documents/papers/mittal18irosws.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving technology that can enable identification of survivors under collapsed buildings in the aftermath of natural disasters such as earthquakes or gas explosions. However, these UAVs have to be able to autonomously land on debris piles in order to accurately locate the survivors. This problem is extremely challenging as the structure of these debris piles is often unknown and no prior knowledge can be leveraged. In this work, we propose a computationally efficient system that is able to reliably identify safe landing sites and autonomously perform the landing maneuver. Specifically, our algorithm computes costmaps based on several hazard factors including terrain flatness, steepness, depth accuracy and energy consumption information. We first estimate dense candidate landing sites from the resulting costmap and then employ clustering to group neighboring sites into a safe landing region. Finally, a minimum-jerk trajectory is computed for landing considering the surrounding obstacles and the UAV dynamics. We demonstrate the efficacy of our system using experiments from a city scale hyperrealistic simulation environment and in real-world scenarios with collapsed buildings.</p>
    </span>
    
  </div>
</div>
</li></ol>



  </article>

  
    <div class="social">
  <span class="contacticon center">
    <a href="mailto:luyang.zhao.gr@dartmouth.edu"><i class="fa fa-envelope-square"></i></a>
    <a href="https://scholar.google.com/citations?user=iVXG-IkAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar-square"></i></a>
    <a href="https://github.com/luyangzhao" target="_blank" title="GitHub"><i class="fa fa-github-square"></i></a>
    <a href="https://www.linkedin.com/in/mayankm-0096" target="_blank" title="LinkedIn"><i class="fa fa-linkedin-square"></i></a>
    <a href="https://twitter.com/mayankm155" target="_blank" title="Twitter"><i class="fa fa-twitter-square"></i></a>
  </span>

  <div class="col three caption">
    
  </div>
</div>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    <center>
      © Copyright 2023 Luyang Zhao.
      Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

      
    </center>
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92958928-1', 'auto');
  ga('send', 'pageview');
</script>


  </body>

</html>
